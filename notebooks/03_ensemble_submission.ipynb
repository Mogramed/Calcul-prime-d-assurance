{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 03 — Ensemble + Submission (V1)\n\nCe notebook:\n- sélectionne les runs finalistes,\n- optimise les poids d’ensemble (>=0, somme=1),\n- valide primaire/secondaire,\n- simule 2000 shake-ups public/private,\n- refit full train des finalistes,\n- exporte `artifacts/submission_v1.csv` (+ `submission.csv`).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nROOT = Path.cwd()\nif not (ROOT / \"src\").exists():\n    ROOT = ROOT.parent\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\n\nfrom src.v1_pipeline import (\n    COARSE_CONFIGS,\n    ensure_dir,\n    load_json,\n    load_train_test,\n    optimize_non_negative_weights,\n    prepare_datasets,\n    fit_full_two_part_predict,\n    fit_calibrator,\n    apply_calibrator,\n    simulate_public_private_shakeup,\n    rmse,\n    build_submission,\n    save_json,\n)\n\nDATA_DIR = ROOT / \"data\"\nARTIFACT_DIR = ensure_dir(ROOT / \"artifacts\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "run_registry = pd.read_csv(ARTIFACT_DIR / \"run_registry.csv\")\noof = pd.read_parquet(ARTIFACT_DIR / \"oof_predictions.parquet\")\nselected_configs = load_json(ARTIFACT_DIR / \"selected_configs.json\")\n\ntrain_raw, test_raw = load_train_test(DATA_DIR)\nbundle = prepare_datasets(train_raw, test_raw, drop_identifiers=True)\n\nrun_metrics = run_registry[run_registry[\"level\"] == \"run\"].copy()\nkey_cols = [\"engine\", \"config_id\", \"seed\", \"severity_mode\", \"calibration\"]\n\ndef mk_run_id(df):\n    return (\n        df[\"engine\"].astype(str) + \"|\" +\n        df[\"config_id\"].astype(str) + \"|\" +\n        df[\"seed\"].astype(str) + \"|\" +\n        df[\"severity_mode\"].astype(str) + \"|\" +\n        df[\"calibration\"].astype(str)\n    )\n\nrun_metrics[\"run_id\"] = mk_run_id(run_metrics)\noof[\"run_id\"] = mk_run_id(oof)\n\nprint(\"Run metrics:\", run_metrics.shape)\nprint(\"OOF rows:\", oof.shape)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Candidate runs: best par moteur sur primaire, avec garde-fou secondaire\nprim = run_metrics[run_metrics[\"split\"] == \"primary_time\"].copy()\nsec = run_metrics[run_metrics[\"split\"] == \"secondary_group\"].copy()\n\nmerged = prim.merge(\n    sec[key_cols + [\"rmse_prime\", \"q99_ratio_pos\"]].rename(\n        columns={\n            \"rmse_prime\": \"rmse_prime_secondary\",\n            \"q99_ratio_pos\": \"q99_ratio_secondary\",\n        }\n    ),\n    on=key_cols,\n    how=\"left\",\n)\nmerged[\"rmse_gap_secondary_minus_primary\"] = (\n    merged[\"rmse_prime_secondary\"] - merged[\"rmse_prime\"]\n)\n\n# limiter aux configs sélectionnées dans notebook 02\nmask_sel = np.zeros(len(merged), dtype=bool)\nfor engine, cfg_ids in selected_configs.items():\n    mask_sel |= (merged[\"engine\"].eq(engine) & merged[\"config_id\"].isin(cfg_ids))\nmerged = merged[mask_sel].copy()\n\nfinalists = []\nfor engine, g in merged.groupby(\"engine\"):\n    # garde-fou overfit secondaire si disponible\n    g_ok = g[g[\"rmse_gap_secondary_minus_primary\"].fillna(0) <= 10.0]\n    if g_ok.empty:\n        g_ok = g\n    best = g_ok.sort_values([\"rmse_prime\", \"rmse_prime_secondary\"]).head(1)\n    finalists.append(best)\nfinalists = pd.concat(finalists, ignore_index=True)\nfinalists[\"run_id\"] = mk_run_id(finalists)\n\nfinalists\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Matrices prédiction pour optimisation des poids\ndef build_matrix(pred_df, split_name, run_ids, is_test=0):\n    d = pred_df[(pred_df[\"split\"] == split_name) & (pred_df[\"is_test\"] == is_test)].copy()\n    d = d[d[\"run_id\"].isin(run_ids)]\n    wide = d.pivot_table(index=\"row_idx\", columns=\"run_id\", values=\"pred_prime\", aggfunc=\"first\")\n    y = (\n        d.groupby(\"row_idx\")[\"y_sev\"].first()\n        if is_test == 0\n        else pd.Series(index=wide.index, dtype=float)\n    )\n    return wide, y\n\nrun_ids = finalists[\"run_id\"].tolist()\n\nXp, yp = build_matrix(oof, \"primary_time\", run_ids, is_test=0)\nmask = Xp.notna().all(axis=1)\nXp_fit = Xp.loc[mask]\nyp_fit = yp.loc[mask]\n\nweights = optimize_non_negative_weights(Xp_fit.values, yp_fit.values)\nweight_map = {rid: float(w) for rid, w in zip(Xp_fit.columns.tolist(), weights)}\nweight_map\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Validation primaire + secondaire + fallback single\nens_primary_pred = Xp_fit.values @ weights\nens_primary_rmse = rmse(yp_fit.values, ens_primary_pred)\n\nXs, ys = build_matrix(oof, \"secondary_group\", run_ids, is_test=0)\nsec_mask = Xs.notna().all(axis=1)\nif sec_mask.any():\n    ens_secondary_pred = Xs.loc[sec_mask].values @ weights\n    ens_secondary_rmse = rmse(ys.loc[sec_mask].values, ens_secondary_pred)\nelse:\n    ens_secondary_rmse = np.nan\n\n# best single (sur secondaire si dispo, sinon primaire)\nif sec_mask.any():\n    single_scores = []\n    for rid in run_ids:\n        p = Xs.loc[sec_mask, rid].values\n        s = rmse(ys.loc[sec_mask].values, p)\n        single_scores.append((rid, s))\nelse:\n    single_scores = []\n    for rid in run_ids:\n        p = Xp_fit[rid].values\n        s = rmse(yp_fit.values, p)\n        single_scores.append((rid, s))\nbest_single_run, best_single_rmse = sorted(single_scores, key=lambda x: x[1])[0]\n\nprint(\"Ensemble RMSE primary:\", round(float(ens_primary_rmse), 6))\nprint(\"Ensemble RMSE secondary:\", round(float(ens_secondary_rmse), 6) if not np.isnan(ens_secondary_rmse) else np.nan)\nprint(\"Best single run:\", best_single_run)\nprint(\"Best single RMSE :\", round(float(best_single_rmse), 6))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Shake-up simulation (public/private)\nshake_ens = simulate_public_private_shakeup(\n    yp_fit.values, ens_primary_pred, n_sim=2000, public_ratio=1/3, seed=42\n)\nshake_single = simulate_public_private_shakeup(\n    yp_fit.values, Xp_fit[best_single_run].values, n_sim=2000, public_ratio=1/3, seed=42\n)\n\nens_gap_std = float(shake_ens[\"gap_public_minus_private\"].std())\nsingle_gap_std = float(shake_single[\"gap_public_minus_private\"].std())\n\n# Décision finale: ensemble seulement s'il n'est pas instable ni pire en secondaire\nuse_ensemble = True\nif not np.isnan(ens_secondary_rmse):\n    if ens_secondary_rmse > best_single_rmse + 1.0:\n        use_ensemble = False\nif ens_gap_std > single_gap_std * 1.05:\n    use_ensemble = False\n\nprint(\"ens_gap_std:\", round(ens_gap_std, 6))\nprint(\"single_gap_std:\", round(single_gap_std, 6))\nprint(\"use_ensemble:\", use_ensemble)\n\nshake_ens.to_parquet(ARTIFACT_DIR / \"shakeup_ensemble.parquet\", index=False)\nshake_single.to_parquet(ARTIFACT_DIR / \"shakeup_single.parquet\", index=False)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Refit full train des runs finalistes puis prédiction test\ncfg_lookup = {\n    engine: {c[\"config_id\"]: c for c in cfgs}\n    for engine, cfgs in COARSE_CONFIGS.items()\n}\n\nfull_test_preds = {}\ny_freq_np = bundle.y_freq.to_numpy(dtype=int)\ny_sev_np = bundle.y_sev.to_numpy(dtype=float)\n\nfor _, r in finalists.iterrows():\n    rid = r[\"run_id\"]\n    engine = r[\"engine\"]\n    config_id = r[\"config_id\"]\n    seed = int(r[\"seed\"])\n    severity_mode = r[\"severity_mode\"]\n    calibration = r[\"calibration\"]\n\n    cfg = cfg_lookup[engine][config_id]\n    freq_raw_te, sev_te = fit_full_two_part_predict(\n        engine=engine,\n        X_train=bundle.X_train,\n        y_freq_train=y_freq_np,\n        y_sev_train=y_sev_np,\n        X_test=bundle.X_test,\n        cat_cols=bundle.cat_cols,\n        seed=seed,\n        severity_mode=severity_mode,\n        freq_params=cfg[\"freq_params\"],\n        sev_params=cfg[\"sev_params\"],\n    )\n\n    if calibration != \"none\":\n        # calibrateur appris sur OOF (split primaire)\n        oof_run = oof[\n            (oof[\"is_test\"] == 0)\n            & (oof[\"split\"] == \"primary_time\")\n            & (oof[\"run_id\"] == rid)\n        ].copy()\n        valid = oof_run[\"pred_freq\"].notna()\n        cal = fit_calibrator(\n            oof_run.loc[valid, \"pred_freq\"].to_numpy(),\n            oof_run.loc[valid, \"y_freq\"].to_numpy(),\n            method=calibration,\n        )\n        freq_te = apply_calibrator(cal, freq_raw_te, method=calibration)\n    else:\n        freq_te = freq_raw_te\n\n    full_test_preds[rid] = np.maximum(freq_te * sev_te, 0.0)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Combinaison finale + export submission\nif use_ensemble:\n    final_runs = Xp_fit.columns.tolist()\n    w = np.array([weight_map[rid] for rid in final_runs], dtype=float)\n    test_matrix = np.column_stack([full_test_preds[rid] for rid in final_runs])\n    pred_final = test_matrix @ w\n    final_strategy = \"ensemble\"\nelse:\n    pred_final = full_test_preds[best_single_run]\n    final_strategy = \"single\"\n\nsubmission = build_submission(test_raw[\"index\"], pred_final)\nsubmission.to_csv(ARTIFACT_DIR / \"submission_v1.csv\", index=False)\nsubmission.to_csv(ARTIFACT_DIR / \"submission.csv\", index=False)\n\nfinal_meta = {\n    \"final_strategy\": final_strategy,\n    \"final_runs\": finalists[\"run_id\"].tolist(),\n    \"weights\": weight_map,\n    \"best_single_run\": best_single_run,\n    \"ens_primary_rmse\": float(ens_primary_rmse),\n    \"ens_secondary_rmse\": float(ens_secondary_rmse) if not np.isnan(ens_secondary_rmse) else None,\n    \"best_single_rmse\": float(best_single_rmse),\n    \"ens_gap_std\": float(ens_gap_std),\n    \"single_gap_std\": float(single_gap_std),\n}\nsave_json(final_meta, ARTIFACT_DIR / \"ensemble_weights_v1.json\")\nfinalists.to_csv(ARTIFACT_DIR / \"finalist_runs.csv\", index=False)\n\nprint(\"Saved:\")\nprint(\"-\", ARTIFACT_DIR / \"submission_v1.csv\")\nprint(\"-\", ARTIFACT_DIR / \"submission.csv\")\nprint(\"-\", ARTIFACT_DIR / \"ensemble_weights_v1.json\")\nprint(\"-\", ARTIFACT_DIR / \"finalist_runs.csv\")\nsubmission.head()\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}